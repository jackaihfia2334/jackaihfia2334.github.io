<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="MYF  ZJU-ISEE">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="MYF  ZJU-ISEE">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="MYF">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>MYF  ZJU-ISEE</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}.beian img{display:inline-block!important}</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">MYF  ZJU-ISEE</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">When the world turns its back on you, you turn your back on the world</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/23/ASformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/yj.png">
      <meta itemprop="name" content="MYF">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MYF  ZJU-ISEE">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/23/ASformer%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">ASformer论文笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-23 23:18:22 / 修改时间：23:20:30" itemprop="dateCreated datePublished" datetime="2022-08-23T23:18:22+08:00">2022-08-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index"><span itemprop="name">Research</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="ASformer-Transformer-for-Action-Segmentation"><a href="#ASformer-Transformer-for-Action-Segmentation" class="headerlink" title="ASformer**: Transformer for Action Segmentation**"></a>ASformer**: Transformer for Action Segmentation**</h3><p>疑问：代码和论文不一致cross-attention部分</p>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>action segmentation task</p>
<p>the potential of Transformer in modeling the relations among elements in sequential data.</p>
<p>several major concerns and their soluntion</p>
<p>1.lack of inductive biases with small training sets</p>
<p>S: bring in the local connectivity inductive priors because of the high locality of features</p>
<p>2.the defificit in processing long input sequence</p>
<p>S:apply a pre-defifined hierarchical representation pattern that effificiently handles long input sequences</p>
<p>3.limitation of the decoder architecture to utilize temporal relations among multiple</p>
<p>action segments to refifine the initial predictions.</p>
<p>S:design the decoder to refifine the initial predictions from the encoder. </p>
<h4 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.<strong>Introduction</strong></h4><p>任务对比：</p>
<p><img src="https://pic3.zhimg.com/80/v2-96160c6ccda823f78099bcce690ce7d2_720w.jpg" alt="img"></p>
<p>action recognization task : classify a short trimmed video into a single action label</p>
<p>action segmentation task（也可以叫做temporal action detection、temporal action localization） : assign an action label for each frame for a minutes-long untrimmed video</p>
<p>TAL-task一般使用计算好的特征作为模型输入而不是使用原始视频数据</p>
<p>Instead of using raw RGB video sequences as the input, action segmentation methods operate on pre-extracted frame-wise feature sequences and fo cus on modeling the temporal relations among frames</p>
<p>Transformer善于处理序列数据，善于对序列数据中的各个元素间的关系建模</p>
<p>Transformers, originally designed for the machine translation task [41], have achieved great performance for almost all natural language processing(NLP) tasks over the past yearsAction segmentation task is similar to NLP tasks, since both of them are sequence to-sequence prediction tasks</p>
<p>使用原始transformer的问题以及本文提出的对应的解决方法</p>
<p>three major concerns</p>
<p>1.缺乏有效的先验假设（归纳偏置），虽然使得函数集扩大，但需要更多的数据。而本领域的数据集太小</p>
<p>S：<strong>local connectivity inductive bias</strong> .（because every action occupies continued timestamps）</p>
<p>  We bring in such strong inductive priors by <strong><u>applying additional temporal convolutions</u></strong> in each layer.</p>
<p>2.长视频缺乏自注意力，几千帧，自注意力层很难学习到合适的权重。而且导致各个自注意力层无法合作。</p>
<p>S：constraint each self-attention layer with a <strong>pre-defifined hierarchical resentation pattern</strong>,</p>
<p>我们用预先预定义的层次表示模式约束每个自注意层，迫使低层次自注意层首先关注局部关系，然后逐渐扩大它们的足迹，以捕获高层中更长的依赖关系。</p>
<p>3.原始的decoder无法满足此任务的需求，无法优化初始的预测。priorwork都是在encoder后接一个额外的TCN（temporal convolution network）、GCN（graph-based temporal network)</p>
<p>本文贡献总结：To summarize, the main contributions of this work include: 1) An exploration for Transformer on action segmentation task with three distinctive characteristics: the explicitly introduced local connectivity inductive bias, pre-defifined hierarchical representation pattern, and new design of the decoder; 2) state of-the-art action segmentation results on three public datasets.</p>
<p><img src="C:\Users\ys\AppData\Roaming\Typora\typora-user-images\image-20211129155215256.png" alt="image-20211129155215256"></p>
<h4 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2.Related Work"></a>2.<strong>Related Work</strong></h4><h5 id="Action-Segmentation："><a href="#Action-Segmentation：" class="headerlink" title="Action Segmentation："></a><strong>Action Segmentation</strong>：</h5><p>早期采用滑窗方法、时序建模（CRF、HMM、RNN）  ——&gt;  classify framewise actions</p>
<p>近期 temporal convolution及其变体   encoder-decoder temporal convolution , deformable temporal convolution，dilated temporal convolution…….</p>
<p>However, a single convolutional layer does not connect all pairs of input and output positions, which remains room for improvement.</p>
<p><strong>Transformer</strong>:</p>
<p>More recently, some researches study the effificient version of Transformer models, which explore<br> attention restrictions to local windows, such as Swin, BigBird.</p>
<p><strong>Action Detection(存在疑问）</strong> Different from action segmentation task, the action detection task aims</p>
<p>at localizing the start&#x2F;end of the action and recognizing it in a untrimmed video.</p>
<p>分为两种主流方式：一步的（端到端，类比目标检测）、两步的（先生成proposal，再对其分类）</p>
<p>The One stage methods [25, 26] draw on the SSD [27] method in object detection and design end-to-end action detection networks with the similar feature pyramid structures. Two-stage methods adopt the Faster-RCNN [31] architecture, including proposal generation and proposal classifification.</p>
<p><strong><u>action detection 和action segemention的区别在哪里？</u></strong></p>
<p>action segmentation methods predict what action is oc-curring at every frame in a video and detection methods out-put a sparse set action segments, where a segment is definedby a start time, end time, and class label. It is possible toconvert between a given segmentation and set of detectionsby simply adding or removing null&#x2F;background segments.</p>
<p><strong>行为细分</strong>（Action Segmentation）方法预测在一个视频中每一帧出现什么动作。<br><strong>检测</strong>（Detection）方法输出一个稀疏的动作细分集合，这个集合中一个细分由起始时间，和类别标签定义。</p>
<h4 id="3-Methods"><a href="#3-Methods" class="headerlink" title="3.Methods"></a>3.<strong>Methods</strong></h4><h5 id="3-1-Encoder"><a href="#3-1-Encoder" class="headerlink" title="3.1 Encoder"></a><strong>3.1 Encoder</strong></h5><p>输入：the pre-extracted feature sequences of size <em>T</em> <em>×D</em>, where <em>T</em> is the video length and <em>D</em> is the feature dimension</p>
<p>首先是线性映射层，然后接N个block，然后接一个FCN来进行多分类</p>
<p>每个block有两个sub-layer：前馈层和一个单头注意力层仍然使用了残差连接，LN换成了IN，前馈层由全连接层换成了膨胀时序卷积（dilated temporal convolution）</p>
<p>Each encoder block contains two sub-layers. The fifirst is a feed-forward layer, and the second is a single-head self-attention layer. We employ a residual connection around each of the two sub-layers, followed by instance normalization and ReLU activation</p>
<p><strong><u>膨胀卷积的采样间隔数d和受约束的self-attention层的window大小随着层数呈指数型增长，逐步提升感受野</u></strong></p>
<p>Motivated by the success of such a hierarchical pattern, we constraint the receptive fields of each self-attention layer within a local window with size <em>w</em> (<em>e.g</em>. for a frame <em>t</em>, we only calculate the attention weights with the frames that within its local window). The size of the local window is then doubled at each layer <em>i</em> (i.e.,<em>w</em> &#x3D; 2<em>i</em><em>,<em><em>i</em> &#x3D; 1</em>,<em>2</em>…</em>). Meanwhile, we also double the dilation rate of the temporal convolution layer with the encoder depth increasing, keeping consistent with the self-attention layer.</p>
<p><strong>3.2 Decoder</strong></p>
<p>输入：the initial predictions output by the encoder（encoder最后的FCN的输出）</p>
<p>首先也是一个fcn(线性映射层，调整特征维度)，然后接N个block（与encoder基本一致）</p>
<p>Similar to encoder, we use temporal convolution as the feed-forward layer and the hierarchical pattern is also applied for the cross-attention layer.                                                                                  </p>
<p>区别：与自注意层相比，交叉注意有以下区别：查询Q和键K从编码器和前一层的输出连接得到，而值V仅从前一层的输出得到。交叉注意机制允许编码器中的每个位置通过生成注意权重来关注细化过程中的所有位置。</p>
<p>特征空间V完全由输入预测转换，不会被编码器的参与者干扰，因为生成的注意权重只用于执行线性组合。</p>
<p>设计灵感来自于工作[11]，他们表明细化过程对预测的学习特征空间的干扰非常敏感。</p>
<p><strong>3.3 Loss Function &amp; Implementation details</strong></p>
<p>最后的ASformer由一个编码器和三个解码器组成，而每个编码器和解码器包含9个block。将编码器和解码器中的第一个完全连接层的尺寸设置为64，以及每个编码器和解码器块中的特征尺寸。此外，对编码器的输入特征应用一个特殊的dropout层，随机下降整个特征通道，dropout rate为0.3。在所有实验中，我们通过Adam优化器训练模型，学习速率为0.0005。</p>
<h4 id="4-Dataset"><a href="#4-Dataset" class="headerlink" title="4 Dataset"></a><strong>4 Dataset</strong></h4><p>数据集：50Salads   GTEA    Breakfast</p>
<p>预先计算的特征：use the I3D [2] model, which is trained on kinetics [2] dataset, to pre-extract feature sequences. The dimension of the I3D feature for each frame is 2048-d. The following three evaluation metrics are used</p>
<p>评估指标： evaluate the performance: frame-wise accuracy(Acc.), segmental edit score (Edit), and</p>
<p>segmental overlap F1 score with threshold <em>k</em>&#x2F;100, denoted as F1@*k</p>
<h4 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5 Experiments"></a>5 Experiments</h4><h4 id="基本概念补充："><a href="#基本概念补充：" class="headerlink" title="基本概念补充："></a>基本概念补充：</h4><p>Instance Normalization</p>
<p>对于图像风格迁移](<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/55948352)[2]%E8%BF%99%E7%B1%BB%E7%9A%84%E6%B3%A8%E9%87%8D%E6%AF%8F%E4%B8%AA%E5%83%8F%E7%B4%A0%E7%9A%84%E4%BB%BB%E5%8A%A1%E6%9D%A5%E8%AF%B4%EF%BC%8C%E6%AF%8F%E4%B8%AA%E6%A0%B7%E6%9C%AC%E7%9A%84%E6%AF%8F%E4%B8%AA%E5%83%8F%E7%B4%A0%E7%82%B9%E7%9A%84%E4%BF%A1%E6%81%AF%E9%83%BD%E6%98%AF%E9%9D%9E%E5%B8%B8%E9%87%8D%E8%A6%81%E7%9A%84%EF%BC%8C%E4%BA%8E%E6%98%AF%E5%83%8F[BN](https://zhuanlan.zhihu.com/p/54171297)[3]%E8%BF%99%E7%A7%8D%E6%AF%8F%E4%B8%AA%E6%89%B9%E9%87%8F%E7%9A%84%E6%89%80%E6%9C%89%E6%A0%B7%E6%9C%AC%E9%83%BD%E5%81%9A%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E7%AE%97%E6%B3%95%E5%B0%B1%E4%B8%8D%E5%A4%AA%E9%80%82%E7%94%A8%E4%BA%86%EF%BC%8C%E5%9B%A0%E4%B8%BABN%E8%AE%A1%E7%AE%97%E5%BD%92%E4%B8%80%E5%8C%96%E7%BB%9F%E8%AE%A1%E9%87%8F%E6%97%B6%E8%80%83%E8%99%91%E4%BA%86%E4%B8%80%E4%B8%AA%E6%89%B9%E9%87%8F%E4%B8%AD%E6%89%80%E6%9C%89%E5%9B%BE%E7%89%87%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E4%BB%8E%E8%80%8C%E9%80%A0%E6%88%90%E4%BA%86%E6%AF%8F%E4%B8%AA%E6%A0%B7%E6%9C%AC%E7%8B%AC%E7%89%B9%E7%BB%86%E8%8A%82%E7%9A%84%E4%B8%A2%E5%A4%B1%E3%80%82%E5%90%8C%E7%90%86%E5%AF%B9%E4%BA%8E[LN](https://zhuanlan.zhihu.com/p/54530247)[4]%E8%BF%99%E7%B1%BB%E9%9C%80%E8%A6%81%E8%80%83%E8%99%91%E4%B8%80%E4%B8%AA%E6%A0%B7%E6%9C%AC%E6%89%80%E6%9C%89%E9%80%9A%E9%81%93%E7%9A%84%E7%AE%97%E6%B3%95%E6%9D%A5%E8%AF%B4%E5%8F%AF%E8%83%BD%E5%BF%BD%E7%95%A5%E4%BA%86%E4%B8%8D%E5%90%8C%E9%80%9A%E9%81%93%E7%9A%84%E5%B7%AE%E5%BC%82%EF%BC%8C%E4%B9%9F%E4%B8%8D%E5%A4%AA%E9%80%82%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E8%BF%99%E7%B1%BB%E5%BA%94%E7%94%A8%E3%80%82">https://zhuanlan.zhihu.com/p/55948352)[2]这类的注重每个像素的任务来说，每个样本的每个像素点的信息都是非常重要的，于是像[BN](https://zhuanlan.zhihu.com/p/54171297)[3]这种每个批量的所有样本都做归一化的算法就不太适用了，因为BN计算归一化统计量时考虑了一个批量中所有图片的内容，从而造成了每个样本独特细节的丢失。同理对于[LN](https://zhuanlan.zhihu.com/p/54530247)[4]这类需要考虑一个样本所有通道的算法来说可能忽略了不同通道的差异，也不太适用于图像风格迁移这类应用。</a></p>
<p>所以这篇文章提出了Instance Normalization（IN），一种更适合对单个像素有更高要求的场景的归一化算法（IST，GAN等）。IN的算法非常简单，计算归一化统计量时考虑单个样本，单个通道的所有元素。IN（右）和BN（中）以及LN（左）的不同从图1中可以非常明显的看出。（F</p>
<p><img src="https://pic3.zhimg.com/v2-94c40b6f6f41e45f5d254906d70c10ee_r.jpg" alt="preview"></p>
<p>在图1中 <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> 表示样本轴， <img src="https://www.zhihu.com/equation?tex=C" alt="[公式]"> 表示通道轴， <img src="https://www.zhihu.com/equation?tex=F" alt="[公式]"> 是每个通道的特征数量。</p>
<p>时序卷积网络（Temporal convolutional network， TCN）</p>
<p>link:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/150753149">https://zhuanlan.zhihu.com/p/150753149</a></p>
<h1 id="一、引言"><a href="#一、引言" class="headerlink" title="一、引言"></a>一、引言</h1><p>​    时序问题的建模大家一般习惯性的采用循环神经网络（RNN）来建模，这是因为RNN天生的循环自回归的结构是对时间序列的很好的表示。传统的卷积神经网络一般认为不太适合时序问题的建模，这主要由于其卷积核大小的限制，不能很好的抓取长时的依赖信息。 但是最近也有很多的工作显示，特定的卷积神经网络结构也可以达到很好的效果，比如Goolgle提出的用来做语音合成的wavenet，Facebook提出的用来做翻译的卷积神经网络。这就带来一个问题，用卷积来做神经网络到底是只适用于特定的领域还是一种普适的模型？ 本文就带着这个问题，将一种特殊的卷积神经网络——时序卷积网络（Temporal convolutional network， TCN）与多种RNN结构相对比，发现在多种任务上TCN都能达到甚至超过RNN模型。</p>
<h1 id="二、时序卷积神经网络"><a href="#二、时序卷积神经网络" class="headerlink" title="二、时序卷积神经网络"></a>二、时序卷积神经网络</h1><h2 id="2-1-因果卷积（Causal-Convolution）"><a href="#2-1-因果卷积（Causal-Convolution）" class="headerlink" title="2.1 因果卷积（Causal Convolution）"></a>2.1 因果卷积（Causal Convolution）</h2><p>​           <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMzLnpoaW1nLmNvbS92Mi1mMGIzZTQ1MDUwZjczMzFmMmJjODQzNDczZDIxNGM0YV9yLmpwZw?x-oss-process=image/format,png" alt="preview"></p>
<p>​    因果卷积可以用上图直观表示。 即对于上一层t时刻的值，只依赖于下一层t时刻及其之前的值。和传统的卷积神经网络的不同之处在于，因果卷积不能看到未来的数据，它是单向的结构，不是双向的。也就是说只有有了前面的因才有后面的果，是一种严格的时间约束模型，因此被成为因果卷积。</p>
<h2 id="2-2-膨胀卷积（Dilated-Convolution）"><a href="#2-2-膨胀卷积（Dilated-Convolution）" class="headerlink" title="2.2 膨胀卷积（Dilated Convolution）"></a>2.2 膨胀卷积（Dilated Convolution）</h2><p>​    单纯的因果卷积还是存在传统卷积神经网络的问题，即对时间的建模长度受限于卷积核大小的，如果要想抓去更长的依赖关系，就需要线性的堆叠很多的层。为了解决这个问题，研究人员提出了膨胀卷积。如下图所示。</p>
<p>​                 <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMyLnpoaW1nLmNvbS92Mi0wMDgzYmVhY2E2NDZkMTNjM2RjM2UzMzU5OGMwNzY2ZF9yLmpwZw?x-oss-process=image/format,png" alt="preview"></p>
<p>​    和传统卷积不同的是，膨胀卷积允许卷积时的输入存在间隔采样，采样率受图中的d控制。 最下面一层的d&#x3D;1，表示输入时每个点都采样，中间层d&#x3D;2，表示输入时每2个点采样一个作为输入。一般来讲，越高的层级使用的d的大小越大。所以，膨胀卷积使得有效窗口的大小随着层数呈指数型增长。这样卷积网络用比较少的层，就可以获得很大的感受野。</p>
<h2 id="2-3-残差链接（Residual-Connections）"><a href="#2-3-残差链接（Residual-Connections）" class="headerlink" title="2.3 残差链接（Residual Connections）"></a>2.3 残差链接（Residual Connections）</h2><p>​        <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMzLnpoaW1nLmNvbS92Mi0zY2U3ZmI3MzlhNmU5ZGQ1ZWQzNDQ1NzVlN2RhZDAxYV9yLmpwZw?x-oss-process=image/format,png" alt="preview"></p>
<p>​    残差链接被证明是训练深层网络的有效方法，它使得网络可以以跨层的方式传递信息。本文构建了一个残差块来代替一层的卷积。如上图所示，一个残差块包含两层的卷积和非线性映射，在每层中还加入了WeightNorm和Dropout来正则化网络。</p>
<h1 id="三、讨论和总结"><a href="#三、讨论和总结" class="headerlink" title="三、讨论和总结"></a>三、讨论和总结</h1><p>​    总体来讲，TCN模型上的创新并不是很大，因果卷积和扩展卷积也并不是本论文提出来，本文主要是将TCN的结构梳理了一下，相比于wavenet中的结构，去掉了门机制，加入了残差结构，并在很多的序列问题上进行了实验。实验效果如下：</p>
<p>​       <img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9waWMyLnpoaW1nLmNvbS84MC92Mi0yMjcxOGQwYzdkZGQwNzk4NWM3N2U5Mjc5ZWRiNDVhZF9oZC5qcGc?x-oss-process=image/format,png" alt="img"></p>
<p>在多个任务上，都比标准的LSTM、GRU等效果好。</p>
<h2 id="1-TCN的优点"><a href="#1-TCN的优点" class="headerlink" title="1. TCN的优点"></a>1. TCN的优点</h2><p>  （1）并行性。当给定一个句子时，TCN可以将句子并行的处理，而不需要像RNN那样顺序的处理。</p>
<p>  （2）灵活的感受野。TCN的感受野的大小受层数、卷积核大小、扩张系数等决定。可以根据不同的任务不同的特性灵活定制。</p>
<p>  （3）稳定的梯度。RNN经常存在梯度消失和梯度爆炸的问题，这主要是由不同时间段上共用参数导致的，和传统卷积神经网络一样，TCN不太存在梯度消失和爆炸问题。</p>
<p>  （4）内存更低。RNN在使用时需要将每步的信息都保存下来，这会占据大量的内存，TCN在一层里面卷积核是共享的，内存使用更低。</p>
<h2 id="2-TCN的缺点"><a href="#2-TCN的缺点" class="headerlink" title="2. TCN的缺点"></a>2. TCN的缺点</h2><p>  （1）TCN 在迁移学习方面可能没有那么强的适应能力。这是因为在不同的领域，模型预测所需要的历史信息量可能是不同的。因此，在将一个模型从一个对记忆信息需求量少的问题迁移到一个需要更长记忆的问题上时，TCN 可能会表现得很差，因为其感受野不够大。</p>
<p>  （2）论文中描述的TCN还是一种单向的结构，在语音识别和语音合成等任务上，纯单向的结构还是相当有用的。但是在文本中大多使用双向的结构，当然将TCN也很容易扩展成双向的结构，不使用因果卷积，使用传统的卷积结构即可。</p>
<p>  （3）TCN毕竟是卷积神经网络的变种，虽然使用扩展卷积可以扩大感受野，但是仍然受到限制，相比于Transformer那种可以任意长度的相关信息都可以抓取到的特性还是差了点。TCN在文本中的应用还有待检验。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/23/%E5%8A%A8%E4%BD%9C%E6%A3%80%E6%B5%8B%E5%88%86%E5%89%B2%E2%80%94%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E8%AF%B4%E6%98%8E/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/yj.png">
      <meta itemprop="name" content="MYF">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MYF  ZJU-ISEE">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/23/%E5%8A%A8%E4%BD%9C%E6%A3%80%E6%B5%8B%E5%88%86%E5%89%B2%E2%80%94%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E8%AF%B4%E6%98%8E/" class="post-title-link" itemprop="url">动作检测分割—评估指标说明</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-23 23:11:55 / 修改时间：23:17:22" itemprop="dateCreated datePublished" datetime="2022-08-23T23:11:55+08:00">2022-08-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index"><span itemprop="name">Research</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="action-detection-x2F-segmentation评估指标总结"><a href="#action-detection-x2F-segmentation评估指标总结" class="headerlink" title="action detection&#x2F;segmentation评估指标总结"></a>action detection&#x2F;segmentation评估指标总结</h4><p>1.mAP:</p>
<p>缺点：mAP is a useful metric for information retrieval tasks like video search, however for many fine-grained action detection applications, such as robotics or video surveillance. we find that results are not indicative of real-world performance. The key issue is that mAP is very sensitive to a con-fidence score assigned to each segment prediction. These confidences are often simply the mean or maximum class score within the frames corresponding to a predicted segment. By computing these confidences in subtly different ways you obtain wildly different results.</p>
<p>对于视频搜索等信息检索任务来说，MAP是一个有用的指标，然而对于许多细粒度的动作检测应用程序，如机器人技术或视频监控，我们发现结果并不能表明真实的性能。关键问题是，mAP对分配给每个段预测的置信度非常敏感。这些置信度通常只是与预测段对应的帧内的平均值或最大类分数。通过不同的的方式计算这些信心，你会得到截然不同的结果。</p>
<p>2.F1@K   <strong>Temporal Convolutional Networks  for Action Segmentation and Detection</strong></p>
<p>link  <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.05267v1#:~:text=of%20temporal%20models%2C%20which%20we%20call%20Temporal%20Convolutional,capture%20long-range%20temporal%20patterns%20whereas%20our%20Dilated%20TCN">https://arxiv.org/abs/1611.05267v1#:~:text=of%20temporal%20models%2C%20which%20we%20call%20Temporal%20Convolutional,capture%20long-range%20temporal%20patterns%20whereas%20our%20Dilated%20TCN</a>   page4-5</p>
<p>特点：it penalizes over-segmentation errors, (2) it does not penalize for minor temporal shifts between the predictions and ground truth, which may have been caused by annotator variability, and (3) scores are dependent on the number actions and not on the duration of each action instance.</p>
<p>3.edit   &amp; 4.overlap</p>
<p><strong>Learning Convolutional Action Primitives for Fine-grained Action Recognition</strong></p>
<p>link：<a target="_blank" rel="noopener" href="http://colinlea.com/docs/pdf/2016_ICRA_CLea.pdf">http://colinlea.com/docs/pdf/2016_ICRA_CLea.pdf</a></p>
<p>page4-5</p>
<p> (如左图)</p>
<p><img src="C:\hexo\source_posts\image\image-20211130182549303.png"></p>
<p>edit注重动作顺序的正确性，有时间上小的偏移无所谓（如右图）</p>
<p><img src="C:\hexo\source_posts\image\image-20211130182746940.png"></p>
<p>原文如下：</p>
<p><img src="C:\hexo\source_posts\image\image-20211130182754906.png"></p>
<p>5.Mof———frame-wise（基于帧的recall）</p>
<p>TP:正确预测的帧</p>
<p>FP：错误预测的帧数    FP&#x3D;总预测帧数-TP</p>
<p>FN：没有被正确预测的GroundTruth的帧数</p>
<p>TN：此类问题中不存在</p>
<p>recall &#x3D; TP&#x2F;(TP+FN)</p>
<p>precisio &#x3D; TP&#x2F;(TP+FP)</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">    <span class="keyword">if</span> per_keystep: </span><br><span class="line">        list_MoF = [] </span><br><span class="line">        list_IoU = [] </span><br><span class="line">        <span class="keyword">for</span> idx_k <span class="keyword">in</span> <span class="built_in">range</span>(Z_gt_perm.shape[<span class="number">1</span>]):             <span class="comment">#[T,K] </span></span><br><span class="line">            pred_k =  Z_pred_perm[:,idx_k] </span><br><span class="line">            gt_k = Z_gt_perm[:,idx_k] </span><br><span class="line"><span class="comment">#since these are binary matrix, the elementwise product is only 1 if and only if the prediction and ground-truth values are both 1 </span></span><br><span class="line">            intersect = np.multiply(pred_k,gt_k)    </span><br><span class="line">            union = np.clip((pred_k+gt_k).astype(np.<span class="built_in">float</span>),<span class="number">0</span>,<span class="number">1</span>)  <span class="comment">#clip这里将数值约束到0-1之间</span></span><br><span class="line">            </span><br><span class="line">            n_intersect = np.<span class="built_in">sum</span>(intersect) </span><br><span class="line">            n_union = np.<span class="built_in">sum</span>(union) </span><br><span class="line">             </span><br><span class="line">            n_gt = np.<span class="built_in">sum</span>(gt_k==<span class="number">1</span>) </span><br><span class="line">             </span><br><span class="line">            <span class="keyword">if</span> n_gt != <span class="number">0</span>: </span><br><span class="line">                MoF_k = n_intersect/n_gt </span><br><span class="line">                IoU_k = n_intersect/n_union </span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">                MoF_k,IoU_k = [-<span class="number">1</span>,-<span class="number">1</span>] </span><br><span class="line">            list_MoF.append(MoF_k) </span><br><span class="line">            list_IoU.append(IoU_k) </span><br><span class="line">         </span><br><span class="line">        arr_MoF = np.array(list_MoF) </span><br><span class="line">        arr_IoU = np.array(list_IoU) </span><br><span class="line">         </span><br><span class="line">        mask = arr_MoF!=-<span class="number">1</span> </span><br><span class="line">        MoF = np.mean(arr_MoF[mask]) </span><br><span class="line">        IoU = np.mean(arr_IoU[mask]) </span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;overall&#x27;</span>) </span><br><span class="line">        <span class="comment">#since these are binary matrix, the elementwise product is only 1 if and only if the prediction and ground-truth values are both 1 </span></span><br><span class="line">        intersect = np.multiply(Z_pred_perm,Z_gt_perm)                  <span class="comment">#实现矩阵的逻辑与</span></span><br><span class="line">        union = np.clip((Z_pred_perm+Z_gt_perm).astype(np.<span class="built_in">float</span>),<span class="number">0</span>,<span class="number">1</span>)   <span class="comment">#实现矩阵的逻辑或</span></span><br><span class="line">         </span><br><span class="line">        n_intersect = np.<span class="built_in">sum</span>(intersect) </span><br><span class="line">        n_union = np.<span class="built_in">sum</span>(union) </span><br><span class="line">         </span><br><span class="line">        n_gt = np.<span class="built_in">sum</span>(Z_gt_perm) </span><br><span class="line">         </span><br><span class="line">        MoF = n_intersect/n_gt </span><br><span class="line">        IoU = n_intersect/n_union </span><br><span class="line">    <span class="keyword">return</span> MoF,IoU</span><br></pre></td></tr></table></figure>





<p>6.IoU——交并比</p>
<p>7.accuracy———frame-wise</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/23/VS-code-SSH%E8%BF%9E%E6%8E%A5%E5%BC%82%E5%B8%B8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/yj.png">
      <meta itemprop="name" content="MYF">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MYF  ZJU-ISEE">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/23/VS-code-SSH%E8%BF%9E%E6%8E%A5%E5%BC%82%E5%B8%B8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/" class="post-title-link" itemprop="url">VS-code SSH连接异常解决方案</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-08-23 22:57:05 / 修改时间：23:00:35" itemprop="dateCreated datePublished" datetime="2022-08-23T22:57:05+08:00">2022-08-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tools/" itemprop="url" rel="index"><span itemprop="name">Tools</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://blog.csdn.net/StarCap/article/details/118709203?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.highlightwordscore&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-1.highlightwordscore">https://blog.csdn.net/StarCap/article/details/118709203?spm=1001.2101.3001.6650.1&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.highlightwordscore&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.highlightwordscore</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/OOFFrankDura/article/details/110492553">https://blog.csdn.net/OOFFrankDura/article/details/110492553</a></p>
<p>winSCP查看隐藏文件夹</p>
<p><a target="_blank" rel="noopener" href="https://jingyan.baidu.com/article/67508eb473bf429cca1ce4b6.html">https://jingyan.baidu.com/article/67508eb473bf429cca1ce4b6.html</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/21/%E6%96%B0%E7%9A%84%E5%BC%80%E5%A7%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/yj.png">
      <meta itemprop="name" content="MYF">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MYF  ZJU-ISEE">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/21/%E6%96%B0%E7%9A%84%E5%BC%80%E5%A7%8B/" class="post-title-link" itemprop="url">新的开始</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-21 16:52:06" itemprop="dateCreated datePublished" datetime="2022-08-21T16:52:06+08:00">2022-08-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-23 00:13:21" itemprop="dateModified" datetime="2022-08-23T00:13:21+08:00">2022-08-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Daily/" itemprop="url" rel="index"><span itemprop="name">Daily</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>走上了这条路，就不能回头了</p>
<p>花了点时间复习了Git，找回了很久不用的个人主页，陆续添加了之前的文章，接下去几天还会修缮一下，增加一下美观程度</p>
<p>以后就作为学习各类技术栈的记录，科研的日志，以及生活的随笔</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/23/%E8%87%AA%E7%9B%91%E7%9D%A3%E9%A2%84%E8%AE%AD%E7%BB%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/yj.png">
      <meta itemprop="name" content="MYF">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MYF  ZJU-ISEE">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/02/23/%E8%87%AA%E7%9B%91%E7%9D%A3%E9%A2%84%E8%AE%AD%E7%BB%83/" class="post-title-link" itemprop="url">自监督预训练</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-23 00:55:38" itemprop="dateCreated datePublished" datetime="2022-02-23T00:55:38+08:00">2022-02-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-23 00:57:35" itemprop="dateModified" datetime="2022-08-23T00:57:35+08:00">2022-08-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-learning/" itemprop="url" rel="index"><span itemprop="name">Deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="自监督预训练"><a href="#自监督预训练" class="headerlink" title="自监督预训练"></a>自监督预训练</h3><p>1.1 Self-supervised Learning<br>在预训练阶段我们使用无标签的数据集 (unlabeled data)，因为有标签的数据集很贵，打标签得要多少人工劳力去标注，那成本是相当高的，太贵。相反，无标签的数据集网上随便到处爬，它便宜。在训练模型参数的时候，我们不追求把这个参数用带标签数据从初始化的一张白纸给一步训练到位，原因就是数据集太贵。于是 Self-Supervised Learning 就想先把参数从一张白纸训练到初步成型，再从初步成型训练到完全成型。注意这是2个阶段。这个训练到初步成型的东西，我们把它叫做 Visual Representation。预训练模型的时候，就是模型参数从一张白纸到初步成型的这个过程，还是用无标签数据集。等我把模型参数训练个八九不离十，这时候再根据你下游任务 (Downstream Tasks) 的不同去用带标签的数据集把参数训练到完全成型，那这时用的数据集量就不用太多了，因为参数经过了第1阶段就已经训练得差不多了。</p>
<p>第一个阶段不涉及任何下游任务，就是拿着一堆无标签的数据去预训练，没有特定的任务，这个话用官方语言表达叫做：in a task-agnostic way。第二个阶段涉及下游任务，就是拿着一堆带标签的数据去在下游任务上 Fine-tune，这个话用官方语言表达叫做：in a task-specific way。</p>
<p>以上这些话就是 Self-Supervised Learning 的核心思想，如下图1所示，后面还会再次提到它。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/500f387fc8f44fba0481140005109af5.png" alt="图1：Self-Supervised Learning 的核心思想"></p>
<p>Self-Supervised Learning 不仅是在NLP领域，在CV, 语音领域也有很多经典的工作，如下图2所示。它可以分成3类：Data Centric, Prediction (也叫 Generative) 和 Contrastive。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/9db77d8f55425d9debd1a4d4f7acbac6.png" alt="图2：各个领域的 Self-Supervised Learning"></p>
<p>其中的主流就是基于 Generative 的方法和基于 Contrative 的方法。如下图 3 所示这里简单介绍下。基于 Generative 的方法主要关注的重建误差，比如对于 NLP 任务而言，一个句子中间盖住一个 token，让模型去预测，令得到的预测结果与真实的 token 之间的误差作为损失。基于 Contrastive 的方法不要求模型能够重建原始输入，而是希望模型能够在特征空间上对不同的输入进行分辨。</p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/0f6759c96a9b8bb4b026c4d29abe5aec.png" alt="图3：基于 generative 的方法和基于 contrastive 的方法的总结图片"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/21/Baseline1-note-code/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/yj.png">
      <meta itemprop="name" content="MYF">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MYF  ZJU-ISEE">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/21/Baseline1-note-code/" class="post-title-link" itemprop="url">毕设Baseline1代码结构和笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-21 16:52:06" itemprop="dateCreated datePublished" datetime="2021-11-21T16:52:06+08:00">2021-11-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-23 23:21:58" itemprop="dateModified" datetime="2022-08-23T23:21:58+08:00">2022-08-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index"><span itemprop="name">Research</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="毕设baseline1代码结构和笔记"><a href="#毕设baseline1代码结构和笔记" class="headerlink" title="毕设baseline1代码结构和笔记"></a>毕设baseline1代码结构和笔记</h3><h4 id="1"><a href="#1" class="headerlink" title="1."></a>1.</h4><p>sub-sample-rate就是新的采样率 ，这里设置为2</p>
<p>CrossTaskDataset中，已经将gt信息写入h5py文件中了</p>
<h4 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h4><p>破案了，在ProceL数据集中有segment的存在，因此有这个方法的应用</p>
<p>attention_summarization中也有这样的操作，aggregated_features将同一片段不同帧的特征mean成一个  T-&gt;S</p>
<p> # 现在的情况就是T&#x3D;S，没有真正意义上的segment的存在，因为提取的feature就是降采样后提取的，feature[1,T,49,521]中的T不是视频原本的帧数</p>
<p>subsample-segment_list以及函数aggregated_keysteps 的作用不明确 </p>
<p> 实验证明经过aggregated_keysteps处理后的key_step_lables和处理前的key_step-list是一致的</p>
<p>函数定义如下</p>
<p>def aggregated_keysteps(subsampled_segment_list, key_step_list):</p>
<p>  “””</p>
<p>  Function which aggregate the subsampled keysteps assigning the keystep which is found in majority for each segment</p>
<p>  Parameters</p>
<p>  -———</p>
<p>  subsampled_segment_list: subsampled list of segments key_step_list: a list denoting the key step number each frame belongs to.</p>
<p>  Returns</p>
<p>  -——</p>
<p>  batch_aggregated_key_list: list denoting which segment in the aggregated</p>
<p>  feature belongs to which key step</p>
<p>  “””-</p>
<h4 id="3-模型中的数据流动"><a href="#3-模型中的数据流动" class="headerlink" title="3.模型中的数据流动"></a>3.模型中的数据流动</h4><h4 id="4-评估方式"><a href="#4-评估方式" class="headerlink" title="4.评估方式"></a>4.评估方式</h4><p> Mof（分为是否per-keystep())</p>
<p>++++++++++++</p>
<p>有应用内置的匈牙利算法实现linear_sum_assignment来实现分配</p>
<p><img src="C:\Users\ys\AppData\Roaming\Typora\typora-user-images\image-20211128205936133.png" alt="image-20211128205936133"></p>
<h4 id="5-bug-and-warning"><a href="#5-bug-and-warning" class="headerlink" title="5 bug and warning"></a>5 bug and warning</h4><p>runtime error</p>
<p>&#x2F;data&#x2F;yufan&#x2F;eccv20&#x2F;core&#x2F;FeatureVGGDataset_CrossTask.py:180: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.</p>
<p>UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.<br>  warnings.warn(“nn.functional.tanh is deprecated. Use torch.tanh instead.”)</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/12/%E4%BB%A3%E7%A0%81%E7%BB%86%E8%8A%82%E9%9A%8F%E6%9C%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/yj.png">
      <meta itemprop="name" content="MYF">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MYF  ZJU-ISEE">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/12/%E4%BB%A3%E7%A0%81%E7%BB%86%E8%8A%82%E9%9A%8F%E6%9C%BA/" class="post-title-link" itemprop="url">代码细节随记载</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-12 22:47:20" itemprop="dateCreated datePublished" datetime="2021-11-12T22:47:20+08:00">2021-11-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-23 23:17:32" itemprop="dateModified" datetime="2022-08-23T23:17:32+08:00">2022-08-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/Pytorch/" itemprop="url" rel="index"><span itemprop="name">Pytorch</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h4><h5 id="1-functools-partial详解"><a href="#1-functools-partial详解" class="headerlink" title="1.functools partial详解"></a>1.functools partial详解</h5><p>首先从一个例子说起：</p>
<p><img src="https://img-blog.csdn.net/20170804092530371" alt="img"></p>
<p>首先我们定义了一个function add ，它接收两个参数a和b，返回a和b的和。然后我们使用partial ，第一个参数是fun ，即传入我们的函数add，然后 再传入一个参数 ，这里是 1 ，它返回给我们一个新的function （addOne）。我们发现这个新的function 只需要接受一个参数，然后返回这个参数与 1 的和。看起来其实相当于我们在使用add函数时固定了a的值为1（注意是从左到右固定参数的值，你可以自己定义一个减法测试）。那我们是不是 可以固定多个参数的，当然是可以的，看下面这个例子：<img src="https://img-blog.csdn.net/20170804093505666" alt="img"></p>
<p>可以看出我们是可以固定多个参数的，如上面的addTwo函数相对于原函数add固定了a和b的值 。 </p>
<p>有了初步的印象，接下来让我们来看看官方给出的定义：</p>
<p><img src="https://img-blog.csdn.net/20170804093904291" alt="img"></p>
<p>其中*args 和 <strong>keywords 表示参数不定的情况：*args表示任何多个无名参数,它是一个tuple；</strong>keywords表示关键字参数,它是一个dict。 看起来有些复杂，我们忽略**keywords，其实这个函数的功能类似于这样：</p>
<p><img src="https://img-blog.csdn.net/20170804095647822" alt="img"></p>
<p>我们使用print来打印参数以看下到底是个怎样的流程：</p>
<p><img src="https://img-blog.csdn.net/20170804101518145" alt="img"></p>
<p>实际上我们是固定了a的值为2，b的值为3，当传入4的时候相当于c的值为4。你可以把官方给出的partial函数copy到console，加上print来 查看更详细的过程。这里只简要举个类似的情况。</p>
<p>举个粒子~~我们来瞧瞧为什么要使用它：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> RND</span><br><span class="line">fnx = <span class="keyword">lambda</span>: RND.randint(<span class="number">0</span>, <span class="number">10</span>)</span><br><span class="line">data = [ (fnx(), fnx()) <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>) ]</span><br><span class="line">target = (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">euclid_dist</span>(<span class="params">v1, v2</span>):</span><br><span class="line">    x1, y1 = v1</span><br><span class="line">    x2, y2 = v2</span><br><span class="line">    <span class="keyword">return</span> math.sqrt((x2 - x1)**<span class="number">2</span> + (y2 - y1)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>其中data中包含了一系列的点的坐标，我们想要计算这些点到target点的距离并且进行排序。<br>于是我们使用sort函数 ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.sort(key=euclid_dist)</span><br></pre></td></tr></table></figure>

<p>但是报错如下：</p>
<p><img src="https://img-blog.csdn.net/20170804103754439" alt="img"></p>
<p>sort函数只接受这样一个函数——这个函数只有一个参数。<br>于是乎我们的partial就派上用场了：<img src="https://img-blog.csdn.net/20170804103924888" alt="img"></p>
<h4 id="pandas"><a href="#pandas" class="headerlink" title="pandas"></a>pandas</h4><h5 id="1-df-iloc"><a href="#1-df-iloc" class="headerlink" title="1.df.iloc[]"></a>1.df.iloc[]</h5><p>iloc是基于整数型位置参数来进行检索，也可以是一个布尔值数组<br>允许的参数有：</p>
<p>先构建一个实例<br>import pandas as pd<br>import matplotlib.pyplot as plt<br>df &#x3D; pd.DataFrame(index&#x3D;[‘小明’,’小红’,’小王’],data&#x3D;{‘height’:[178,171,185],’weight’:[156,90,140]})<br>df<br>        height	weight<br>小明	178		156<br>小红	171		90<br>小王	185		140<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>一个整数<br>df.iloc[2] #如果只有一个参数，那么默认是行</p>
<p>height    185<br>weight    140<br>Name: 小王, dtype: int64<br>1<br>2<br>3<br>4<br>5<br>一个数组<br>df.iloc[[0,2]]</p>
<pre><code>height	weight
</code></pre>
<p>小明	178	156<br>小王	185	140<br>1<br>2<br>3<br>4<br>5<br>一个整数的切片<br>df.iloc[1:2]#依旧是左闭右开</p>
<pre><code>height	weight
</code></pre>
<p>小红	171	90<br>1<br>2<br>3<br>4<br>一个布尔数组<br>df.iloc[[True,False,True]]#数组长度要和数据集长度匹配</p>
<pre><code>height	weight
</code></pre>
<p>小明	178	156<br>小王	185	140<br>1<br>2<br>3<br>4<br>5<br>函数<br>df.iloc[lambda x: x.index &#x3D;&#x3D; ‘小王’]</p>
<pre><code>height	weight
</code></pre>
<p>小王	185	140</p>
<h4 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h4><h5 id="nn-LayerNorm"><a href="#nn-LayerNorm" class="headerlink" title="nn.LayerNorm"></a>nn.LayerNorm</h5><p>LayerNorm参数<br>torch.nn.LayerNorm(<br>        normalized_shape: Union[int, List[int], torch.Size],<br>        eps: float &#x3D; 1e-05,<br>        elementwise_affine: bool &#x3D; True)<br>normalized_shape<br>如果传入整数，比如4，则被看做只有一个整数的list，此时LayerNorm会对输入的最后一维进行归一化，这个int值需要和输入的最后一维一样大。</p>
<p>假设此时输入的数据维度是[3, 4]，则对3个长度为4的向量求均值方差，得到3个均值和3个方差，分别对这3行进行归一化（每一行的4个数字都是均值为0，方差为1）；LayerNorm中的weight和bias也分别包含4个数字，重复使用3次，对每一行进行仿射变换（仿射变换即乘以weight中对应的数字后，然后加bias中对应的数字），并会在反向传播时得到学习。<br>如果输入的是个list或者torch.Size，比如[3, 4]或torch.Size([3, 4])，则会对网络最后的两维进行归一化，且要求输入数据的最后两维尺寸也是[3, 4]。</p>
<p>假设此时输入的数据维度也是[3, 4]，首先对这12个数字求均值和方差，然后归一化这个12个数字；weight和bias也分别包含12个数字，分别对12个归一化后的数字进行仿射变换（仿射变换即乘以weight中对应的数字后，然后加bias中对应的数字），并会在反向传播时得到学习。<br>假设此时输入的数据维度是[N, 3, 4]，则对着N个[3,4]做和上述一样的操作，只是此时做仿射变换时，weight和bias被重复用了N次。<br>假设此时输入的数据维度是[N, T, 3, 4]，也是一样的，维度可以更多。<br>注意：显然LayerNorm中weight和bias的shape就是传入的normalized_shape。</p>
<p>eps<br>归一化时加在分母上防止除零。</p>
<p>elementwise_affine<br>如果设为False，则LayerNorm层不含有任何可学习参数。</p>
<p>如果设为True（默认是True）则会包含可学习参数weight和bias，用于仿射变换，即对输入数据归一化到均值0方差1后，乘以weight，即bias。</p>
<h5 id="torch-roll"><a href="#torch-roll" class="headerlink" title="torch.roll"></a>torch.roll</h5><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># swin transformer中移位的操作</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CyclicShift</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"><span class="variable language_">self</span>, displacement</span>):</span><br><span class="line">        <span class="variable language_">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.displacement = displacement</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"><span class="variable language_">self</span>, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.roll(x, shifts=(<span class="variable language_">self</span>.displacement, <span class="variable language_">self</span>.displacement), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<p>torch.roll(input, shifts, dims&#x3D;None) 这个函数到底是干啥的，咋用的呢？<br> 简单说，它就是用来移位的，是顺移。input是咱们要移动的tensor向量，shifts是要移动到的位置，要移动去哪儿，<strong>dims是在什么维度去移动(这里的dim跟一般操作有所不同，以二维张量为例，dim0表示行，dim1表示列）</strong>如2维的数据，那就两个方向，横着或者竖着。对了，关键的一句话，所有操作针对第一行或者第一列，主要是这个”第一”，下面举例子给大家做解释，自己慢慢体会</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.roll(x, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">tensor([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>

<p>torch.roll(x, 1, 0) 这行代码的意思就是把x的第一行(0维度)移到1这个位置上，其他位置的数据顺移。<br> x——咱们要移动的向量<br> 1——第一行向量要移动到的最终位置<br> 0——从行的角度去移动</p>
<p>再来一个列的例子</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.roll(x, -<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">8</span>, <span class="number">9</span>, <span class="number">7</span>]])</span><br></pre></td></tr></table></figure>

<p>torch.roll(x, -1, 1) 这行代码的意思就是把x的第一列(1维度)移到-1这个位置(最后一个位置)上，其他位置的数据顺移。</p>
<p>shifts和dims可以是元组，其实就是分步骤去移动，再举个例子</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch.roll(x, (<span class="number">0</span>,<span class="number">1</span>), (<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">tensor([[<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">9</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>

<p>torch.roll(x, (0,1), (1,1)) 这行代码的意思：<br> 第一步，把x的第一列(1维度)移到0这个位置(原地不动)上，其他位置的数据顺移。(所有数据原地不动)</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">&gt;&gt;&gt; x</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; a = torch.roll(x, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br></pre></td></tr></table></figure>

<p>第二步，把a的第一列(1维度)移到1这个位置上，其他位置的数据顺移。</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.roll(x, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; b = torch.roll(a, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">&gt;&gt;&gt; b</span><br><span class="line">tensor([[<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">9</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br></pre></td></tr></table></figure>



<h5 id="permute函数"><a href="#permute函数" class="headerlink" title="permute函数"></a>permute函数</h5><h5 id="contiguous函数"><a href="#contiguous函数" class="headerlink" title="contiguous函数"></a>contiguous函数</h5><p><strong>1 先看看官方中英文doc：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.contiguous (Python method, in torch.Tensor)</span><br><span class="line">torch.Tensor.is_contiguous (Python method, in torch.Tensor)</span><br></pre></td></tr></table></figure>

<p><strong>1.1 contiguous() → Tensor</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Returns a contiguous tensor containing the same data as self tensor. If self tensor is contiguous, this function returns the self tensor.</span><br></pre></td></tr></table></figure>

<p><strong>1.2 contiguous() → Tensor</strong></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">返回一个内存连续的有相同数据的tensor，如果原tensor内存连续，则返回原tensor；</span><br></pre></td></tr></table></figure>

<p><strong>2 <a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=pytorch+contiguous&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:64376950%7D">pytorch contiguous</a>的使用</strong></p>
<p>contiguous一般与transpose，permute，view搭配使用：使用transpose或permute进行维度变换后，调用contiguous，然后方可使用view对维度进行变形（如：tensor_var.contiguous().view() ），示例如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor(2,3)</span><br><span class="line">y = x.permute(1,0)         # permute：二维tensor的维度变换，此处功能相当于转置transpose</span><br><span class="line">y.view(-1)                 # 报错，view使用前需调用contiguous()函数</span><br><span class="line">y = x.permute(1,0).contiguous()</span><br><span class="line">y.view(-1)                 # OK</span><br></pre></td></tr></table></figure>

<p>具体原因有两种说法：</p>
<p>1 transpose、permute等维度变换操作后，tensor在内存中不再是连续存储的，而view操作要求tensor的内存连续存储，所以需要contiguous来返回一个contiguous copy；</p>
<p>2 维度变换后的变量是之前变量的浅拷贝，指向同一区域，即view操作会连带原来的变量一同变形，这是不合法的，所以也会报错；—- 这个解释有部分道理，也即contiguous返回了tensor的深拷贝contiguous copy数据；</p>
<p>3 contiguous函数分析，参考CSDN博客</p>
<p>在pytorch中，只有很少几个操作是不改变tensor的内容本身，而只是重新定义下标与元素的对应关系。换句话说，这种操作不进行数据拷贝和数据的改变，变的是元数据，这些操作是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">narrow()，view()，expand()，transpose()；</span><br></pre></td></tr></table></figure>

<p>举个栗子，在使用transpose()进行转置操作时，pytorch并不会创建新的、转置后的tensor，而是修改了tensor中的一些属性（也就是元数据），使得此时的offset和stride是与转置tensor相对应的，而转置的tensor和原tensor的内存是共享的！</p>
<p>为了证明这一点，我们来看下面的代码：****</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(3, 2)</span><br><span class="line">y = x.transpose(x, 0, 1)</span><br><span class="line">x[0, 0] = 233</span><br><span class="line">print(y[0, 0])       # print 233</span><br></pre></td></tr></table></figure>

<p>可以看到，改变了x的元素的值的同时，y的元素的值也发生了变化；也即，经过上述操作后得到的tensor，它内部数据的布局方式和从头开始创建一个常规的tensor的布局方式是不一样的！于是就有contiguous()的用武之地了。</p>
<p>在上面的例子中，x是contiguous的，但y不是（因为内部数据不是通常的布局方式）。注意：不要被contiguous的字面意思“连续的”误解，tensor中数据还是在内存中一块区域里，只是布局的问题！</p>
<p>*当调用contiguous()时，会强制拷贝一份tensor，让它的布局和从头创建的一模一样；*</p>
<p>一般来说这一点不用太担心，如果你没在需要调用contiguous()的地方调用contiguous()，运行时会提示你：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: input is not contiguous</span><br></pre></td></tr></table></figure>

<p>只要看到这个错误提示，加上contiguous()就好啦～</p>
<h5 id="flatten-参数详解"><a href="#flatten-参数详解" class="headerlink" title="flatten()参数详解"></a>flatten()参数详解</h5><p>flatten()是对多维数据的降维函数。<br>flatten(),默认缺省参数为0，也就是说flatten()和flatte(0)效果一样。<br>python里的flatten(dim)表示，从第dim个维度开始展开，将后面的维度转化为一维.也就是说，只保留dim之前的维度，其他维度的数据全都挤在dim这一维。<br>比如一个数据的维度是( S 0 , S 1 , S 2……… , S n ) (S0,S1,S2………,Sn)(S0,S1,S2………,Sn), flatten(m)后的数据为( S 0 ， S 1 ， S 2 ， . . . ， S m − 2 ， S m − 1 ， S m ∗ S m + 1 ∗ S m + 2 ∗ . . . ∗ S n ) (S0，S1，S2，…，Sm-2，Sm-1，Sm<em>Sm+1</em>Sm+2*…*Sn)(S0，S1，S2，…，Sm−2，Sm−1，Sm∗Sm+1∗Sm+2∗…∗Sn)</p>
<p> nn.AdaptiveAvgPool1d &amp; nn.AdaptiveAvgPool2d</p>
<p>eg1 </p>
<p><code>m = nn.AdaptiveAvgPool1d(1)</code></p>
<p><code>input = torch.randn(12, 64, 8)</code></p>
<p><code>output = m(input)</code></p>
<p><code>print(output.shape())</code></p>
<p>输出(12，64，1）</p>
<p>eg2</p>
<p><code>m = nn.AdaptiveAvgPool2d((5,6))</code></p>
<p><code>input = torch.randn(12, 64, 8)</code></p>
<p><code>output = m(input)</code></p>
<p><code>print(output.shape())</code></p>
<p>输出(12，5，6）</p>
<h4 id="Conv1d-amp-Conv2d"><a href="#Conv1d-amp-Conv2d" class="headerlink" title="Conv1d&amp;Conv2d"></a>Conv1d&amp;Conv2d</h4><p>输入数据格式如下：</p>
<p><img src="https://img2018.cnblogs.com/blog/1498369/201905/1498369-20190504202338298-1508304575.png" alt="img"></p>
<p>Conv1d类比，输入数据格式是（Batch_szie, Channel, Length of text）</p>
<p>class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride&#x3D;1, padding&#x3D;0, dilation&#x3D;1, groups&#x3D;1, bias&#x3D;True)</p>
<p>in_channels(int) – 输入信号的通道。在文本分类中，即为词向量的维度<br>out_channels(int) – 卷积产生的通道。有多少个out_channels，就需要多少个1维卷积<br>kernel_size(int or tuple) - 卷积核的尺寸，卷积核的大小为(k,)，第二个维度是由in_channels来决定的，所以实际上卷积大小为kernel_size*in_channels<br>stride(int or tuple, optional) - 卷积步长<br>padding (int or tuple, optional)- 输入的每一条边补充0的层数<br>dilation(int or tuple, &#96;optional&#96;&#96;) – 卷积核元素之间的间距<br>groups(int, optional) – 从输入通道到输出通道的阻塞连接数<br>bias(bool, optional) - 如果bias&#x3D;True，添加偏置<br>举个例子:</p>
<p>conv1 &#x3D; nn.Conv1d(in_channels&#x3D;256，out_channels&#x3D;100,kernel_size&#x3D;2)<br>input &#x3D; torch.randn(32,35,256)</p>
<p>batch_size x text_len x embedding_size -&gt; batch_size x embedding_size x text_len</p>
<p>input &#x3D; input.permute(0,2,1)<br>out &#x3D; conv1(input)<br>print(out.size())<br>这里32为batch_size，35为句子最大长度，256为词向量</p>
<p>再输入一维卷积的时候，需要将32<em>35</em>256变换为32<em>256</em>35，因为一维卷积是在最后维度上扫的，最后out的大小即为：32 <em>100</em>（35-2+1）&#x3D;32 <em>100</em> 34</p>
<h4 id="Pytorch中transpose与numpy中transpose的异同"><a href="#Pytorch中transpose与numpy中transpose的异同" class="headerlink" title="Pytorch中transpose与numpy中transpose的异同"></a>Pytorch中transpose与numpy中transpose的异同</h4><p>相同点，都用于作不同维度间的转换，且都是深拷贝，不影响原来的</p>
<h3 id="张量乘法"><a href="#张量乘法" class="headerlink" title="张量乘法"></a>张量乘法</h3><p>  1.torch.bmm </p>
<p><strong>函数作用</strong><br>计算两个tensor的矩阵乘法，torch.bmm(a,b),tensor a 的size为(b,h,w),tensor b的size为(b,w,h),注意两个tensor的维度必须为3</p>
<ol start="2">
<li>torch.matmul</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/2020042710180343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FzbXg2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/20200427102557964.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FzbXg2NjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4 id="PyTorch实现MLP的两种方法，以及nn-Conv1d-kernel-size-x3D-1和nn-Linear的区别"><a href="#PyTorch实现MLP的两种方法，以及nn-Conv1d-kernel-size-x3D-1和nn-Linear的区别" class="headerlink" title="PyTorch实现MLP的两种方法，以及nn.Conv1d, kernel_size&#x3D;1和nn.Linear的区别"></a>PyTorch实现MLP的两种方法，以及nn.Conv1d, kernel_size&#x3D;1和nn.Linear的区别</h4><p>两者可以实现同样结构的MLP计算，但计算形式不同，具体为：</p>
<p>nn.Conv1d输入的是一个[batch, channel, length]，3维tensor，</p>
<p>而nn.Linear输入的是一个[batch, *,in_features]，可变形状tensor，在进行等价计算时务必保证nn.Linear输入tensor为三维</p>
<p>nn.Conv1d作用在第二个维度位置channel，nn.Linear作用在第三个维度位置in_features，对于一个X，若要在两者之间进行等价计算，需要进行tensor.permute，重新排列维度轴秩序</p>
<p>详见博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/l1076604169/article/details/107170146">https://blog.csdn.net/l1076604169/article/details/107170146</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/23/%E6%AF%95%E8%AE%BEbaseline1-paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/yj.png">
      <meta itemprop="name" content="MYF">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MYF  ZJU-ISEE">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/23/%E6%AF%95%E8%AE%BEbaseline1-paper/" class="post-title-link" itemprop="url">毕设baseline1-paper</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-08-23 23:22:32" itemprop="dateCreated datePublished" datetime="2021-08-23T23:22:32+08:00">2021-08-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-23 23:23:36" itemprop="dateModified" datetime="2022-08-23T23:23:36+08:00">2022-08-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index"><span itemprop="name">Research</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>毕设论文笔记1</p>
<h3 id="一：解决什么问题"><a href="#一：解决什么问题" class="headerlink" title="一：解决什么问题"></a>一：解决什么问题</h3><p>procedure learning  （instructionial  video)</p>
<h3 id="二：prior-work"><a href="#二：prior-work" class="headerlink" title="二：prior work"></a>二：prior work</h3><p>1.强监督的，视频有关键步骤的精确标注（精确到起止时间、帧） </p>
<pre><code> goal:分割新的视频或预测未来的关键步骤
</code></pre>
<p>2.弱监督，每个视频给一个关键步骤的list，同时在视频中的每个关键步骤中只标记一帧，从而降低了注释成本，但仍需要人工标注</p>
<p>​	goal:</p>
<p>3.无监督的，利用同一任务的视频结构，以发现和定位视频中的关键步骤 </p>
<p>​	goal:定位视频中的关键步骤，并学习每个关键步骤的模型</p>
<p>​	ps:从视频中的叙述或文本中学习  数据需要人工清理  声音与画面不同步，可能在片头说了所有步骤</p>
<p>现有工作逐渐集中于无监督，仅仅使用视频画面数据  </p>
<p> using a Mallows model [27], joint dynamic summarization [9] or clustering and ordering of visual features </p>
<p>不足之处：</p>
<p>1.集中于单一任务，不同任务需要单独运行（切换），给定一个新任务，无法自动分类。</p>
<p>2.无监督学习涉及许多非凸-优化问题，仍具有挑战性和研究意义</p>
<p>3.大多数现有的无监督过程学习工作都缺乏学习关键步骤发现的信息特征的能力，通常依赖于预先计算的特征。最近的作品集中于联合使用叙述和视觉数据来学习特征，但它们需要访问叙述，并依赖于模式之间的弱对齐。</p>
<h3 id="三：特点（不同于以往）"><a href="#三：特点（不同于以往）" class="headerlink" title="三：特点（不同于以往）"></a>三：特点（不同于以往）</h3><p>无监督的 （没有关键步骤的语言描述、注解；更没有精确到帧、起止时间的标注）subset selection module 作老师 自监督</p>
<p>仅仅使用视频数据本身(没有用到其他，例如声音，字幕等)</p>
<p>多任务的，且用监督学习方法来对新任务进行任务分类（多分类问题）</p>
<p><strong>目标是分类测试视频的任务标签和定位其关键步骤</strong></p>
<h3 id="四：网络结构"><a href="#四：网络结构" class="headerlink" title="四：网络结构"></a>四：网络结构</h3><p>k均值聚类是用EM算法解决的，本身不能反向传播</p>
<p>在纸质论文中分析网络结构的具体内容  结合代码和实验掌握细节设计</p>
<h3 id="五-实验设计"><a href="#五-实验设计" class="headerlink" title="五:实验设计"></a>五:实验设计</h3><p>ss-model是否存在问题？  缺乏时序排列，缺乏匈牙利算法的分配？   查看参考论文【7-9】【31】</p>
<p>评估函数（四个）  有无bg，是否align(后两种没有用到过)</p>
<h3 id="六-实验结果与分析"><a href="#六-实验结果与分析" class="headerlink" title="六:实验结果与分析"></a>六:实验结果与分析</h3><h3 id="other"><a href="#other" class="headerlink" title="other:"></a>other:</h3><p><strong>什么是Embedding？</strong></p>
<p>Embedding（嵌入）是拓扑学里面的词，在深度学习领域经常和Manifold（流形）搭配使用。</p>
<p>可以用几个例子来说明，比如三维空间的球体是一个二维流形嵌入在三维空间（2D manifold embedded in 3D space）。之所以说他是一个二维流形，是因为球上的任意一个点只需要用一个二维的经纬度来表达就可以了。</p>
<p>又比如一个二维空间的旋转矩阵是2x2的矩阵，其实只需要一个角度就能表达了，这就是一个一维流形嵌入在2x2的矩阵空间。</p>
<p><strong>什么是深度学习里的Embedding？</strong></p>
<p>这个概念在深度学习领域最原初的切入点是所谓的<strong>Manifold Hypothesis</strong>（流形假设）。流形假设是指“自然的原始数据是低维的流形嵌入于(embedded in)原始数据所在的高维空间”。那么，深度学习的任务就是把高维原始数据（图像，句子）映射到低维流形，使得高维的原始数据被映射到低维流形之后变得可分，而这个映射就叫嵌入（Embedding）。比如Word Embedding，就是把单词组成的句子映射到一个表征向量。但后来不知咋回事，开始把低维流形的表征向量叫做Embedding，其实是一种误用。。。</p>
<p>如果按照现在深度学习界通用的理解（其实是偏离了原意的），Embedding就是从原始数据提取出来的Feature，也就是那个通过神经网络映射之后的低维向量。</p>
<h4 id=""><a href="#" class="headerlink" title=""></a></h4>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/05/07/convex-optimization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/yj.png">
      <meta itemprop="name" content="MYF">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MYF  ZJU-ISEE">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/07/convex-optimization/" class="post-title-link" itemprop="url">Convex Optimization</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-05-07 14:41:32 / 修改时间：15:00:53" itemprop="dateCreated datePublished" datetime="2020-05-07T14:41:32+08:00">2020-05-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/math/" itemprop="url" rel="index"><span itemprop="name">math</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/math/optimization/" itemprop="url" rel="index"><span itemprop="name">optimization</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>This is a course provided by professor LinQing in USTC, called Optimization theory, but concentreting on convex optimization, which is simpler than non-convex optimizayion problems.So we can also call it simple optimization.The video of lectures are shared in <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Jt411p7jE?from=search&seid=15465159093593254480">bilibili</a>, including 54 lectures.  </p>
<h2 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h2>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2020/05/07/convex-optimization/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/05/05/Machine-Learning-cornerstone/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/yj.png">
      <meta itemprop="name" content="MYF">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MYF  ZJU-ISEE">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/05/Machine-Learning-cornerstone/" class="post-title-link" itemprop="url">Machine Learning (NTU by Lin)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-05-05 00:32:18" itemprop="dateCreated datePublished" datetime="2020-05-05T00:32:18+08:00">2020-05-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-28 09:46:09" itemprop="dateModified" datetime="2020-05-28T09:46:09+08:00">2020-05-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-learning/" itemprop="url" rel="index"><span itemprop="name">Machine learning</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-learning/primary/" itemprop="url" rel="index"><span itemprop="name">primary</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Machine learning, providing by NTU professor Linxuantian.<br>this course includes two parts, ML foundation and ML techniques. The former is about some basic problems and concepts about machine leaning, like how can can machine learing, why can machine learning, when can machine learning and so on… The latter turns to some concrete algorithm (like LDA, SVM, HMM, PCA) and other details. The materials about this courese include videos, slides and some lecture notes from internet.</p>
<h2 id="I-will-share-my-own-summary-and-ideas-here-from-time-to-time"><a href="#I-will-share-my-own-summary-and-ideas-here-from-time-to-time" class="headerlink" title="I will share my own summary and ideas here from time to time."></a>I will share my own summary and ideas here from time to time.</h2>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="MYF"
      src="/images/yj.png">
  <p class="site-author-name" itemprop="name">MYF</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/jackaihfia2334" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;jackaihfia2334" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/31801045@gmail.com" title="E-Mail → 31801045@gmail.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">myf</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
<script src="https://unpkg.com/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: 'unset',
  left: '64px',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#000000',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: '🌓',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>

  <!--动态线条背景-->
  <script type="text/javascript"
  color="220,220,220" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
  </script>

  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/love.js">
  </script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>

